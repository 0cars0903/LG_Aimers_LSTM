{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* 구글 드라이브 내에서 실행하는 폴더 및 파일을 압축하여 메일에 함께 첨부했습니다.  \n",
        "* EDA 및 모델 구현은 Colab 무료버전에서 실행했습니다.  \n",
        "* 첨부한 압축파일을 압축해제하여, 구글 드라이브에 업로드 한 후, 실행할 수 있습니다.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DS9LmOXw2F-H"
      },
      "id": "DS9LmOXw2F-H"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9cQ67xj_GssI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cQ67xj_GssI",
        "outputId": "37d99e03-0333-4bdd-9d61-221af13a7457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/LG해커톤 오프라인\n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브에 저장된 파일을 사용하기 위한 코드\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 저장된 파일의 경로 설정\n",
        "%cd /content/drive/MyDrive/LG해커톤 오프라인"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bnnwMWUs_E0u",
      "metadata": {
        "id": "bnnwMWUs_E0u"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Rwo0m238VoUp",
      "metadata": {
        "id": "Rwo0m238VoUp"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "x7WqL0H3p9R7"
      },
      "id": "x7WqL0H3p9R7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 매출에 큰 영향을 미치는 제품 분석"
      ],
      "metadata": {
        "id": "czdojZpYrByf"
      },
      "id": "czdojZpYrByf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전체 제품 기준"
      ],
      "metadata": {
        "id": "jENr45ZJtt7o"
      },
      "id": "jENr45ZJtt7o"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data/train.csv\")\n",
        "\n",
        "# Drop non-sales related columns to focus on the sales data\n",
        "sales_data = df.drop(columns=['ID', '제품', '대분류', '중분류', '소분류', '브랜드'])\n",
        "\n",
        "# Transpose the DataFrame to have dates as rows and products as columns\n",
        "sales_data_transposed = sales_data.transpose()\n",
        "\n",
        "# Calculate daily sales ranks for each product\n",
        "daily_sales_ranks = sales_data_transposed.rank(axis=1, ascending=False, method='min')\n",
        "\n",
        "# Calculate the daily total sales\n",
        "daily_total_sales = sales_data_transposed.sum(axis=1)\n",
        "\n",
        "# Calculate the 80% threshold for daily total sales\n",
        "eighty_percent_threshold = daily_total_sales * 0.8\n",
        "\n",
        "# Initialize a dictionary to store the rank corresponding to the 80% threshold for each day\n",
        "ranks_at_80_percent = {}\n",
        "\n",
        "# Loop through each day to find the rank that corresponds to the 80% threshold\n",
        "for date, threshold in eighty_percent_threshold.iteritems():\n",
        "    # Get the sorted daily sales for the date\n",
        "    sorted_daily_sales = sales_data_transposed.loc[date].sort_values(ascending=False)\n",
        "\n",
        "    # Calculate the cumulative sum of the sorted daily sales\n",
        "    cumulative_sales = sorted_daily_sales.cumsum()\n",
        "\n",
        "    # Find the rank at which the cumulative sales exceed or meet the 80% threshold\n",
        "    rank_at_80_percent = cumulative_sales[cumulative_sales >= threshold].index[0] + 1  # Adding 1 because index starts at 0\n",
        "\n",
        "    # Store the result in the dictionary\n",
        "    ranks_at_80_percent[date] = rank_at_80_percent\n",
        "\n",
        "# Convert the dictionary to a DataFrame for better visualization\n",
        "df_ranks_at_80_percent = pd.DataFrame(list(ranks_at_80_percent.items()), columns=['Date', 'Rank at 80% Threshold'])\n",
        "\n",
        "# Show the first few rows of the resulting DataFrame\n",
        "df_ranks_at_80_percent.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "awHfkAWKt_ty",
        "outputId": "43f16872-4a2d-4920-afb0-250651bf3573"
      },
      "id": "awHfkAWKt_ty",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c2d1a423ff87>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Calculate daily sales ranks for each product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdaily_sales_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales_data_transposed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Calculate the daily total sales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mrank\u001b[0;34m(self, axis, method, numeric_only, na_option, ascending, pct)\u001b[0m\n\u001b[1;32m   9167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumeric_only\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9168\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9169\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mranker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9170\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9171\u001b[0m                 \u001b[0mnumeric_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mranker\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   9151\u001b[0m                 )\n\u001b[1;32m   9152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9153\u001b[0;31m                 ranks = algos.rank(\n\u001b[0m\u001b[1;32m   9154\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9155\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mrank\u001b[0;34m(values, axis, method, na_option, ascending, pct)\u001b[0m\n\u001b[1;32m   1149\u001b[0m         )\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         ranks = algos.rank_2d(\n\u001b[0m\u001b[1;32m   1152\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/algos.pyx\u001b[0m in \u001b[0;36mpandas._libs.algos.rank_2d\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mlexsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the percentage of total daily sales contributed by the top 20% products for each day\n",
        "percentage_by_top_20 = {}\n",
        "\n",
        "# Loop through each day to find the percentage of total daily sales contributed by the top 20% products\n",
        "for date, total_sales in daily_total_sales.iteritems():\n",
        "    # Get the sorted daily sales for the date\n",
        "    sorted_daily_sales = sales_data_transposed.loc[date].sort_values(ascending=False)\n",
        "\n",
        "    # Calculate the number of top 20% products\n",
        "    num_top_20_products = int(len(sorted_daily_sales) * 0.2)\n",
        "\n",
        "    # Get the sales of the top 20% products and sum them up\n",
        "    top_20_sales_sum = sorted_daily_sales.head(num_top_20_products).sum()\n",
        "\n",
        "    # Calculate the percentage of total daily sales contributed by the top 20% products\n",
        "    percentage = (top_20_sales_sum / total_sales) * 100 if total_sales != 0 else 0\n",
        "\n",
        "    # Store the result in the dictionary\n",
        "    percentage_by_top_20[date] = percentage\n",
        "\n",
        "# Convert the dictionary to a DataFrame for better visualization\n",
        "df_percentage_by_top_20 = pd.DataFrame(list(percentage_by_top_20.items()), columns=['Date', 'Percentage by Top 20%'])\n",
        "\n",
        "# Show the first few rows of the resulting DataFrame\n",
        "df_percentage_by_top_20.head()"
      ],
      "metadata": {
        "id": "mRN6p1w-uEXy"
      },
      "id": "mRN6p1w-uEXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average percentage of total daily sales contributed by the top 20% products\n",
        "average_percentage_by_top_20 = df_percentage_by_top_20['Percentage by Top 20%'].mean()\n",
        "average_percentage_by_top_20"
      ],
      "metadata": {
        "id": "v82L1AqouFh4"
      },
      "id": "v82L1AqouFh4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 그래프 시각화"
      ],
      "metadata": {
        "id": "89L45MziuGnA"
      },
      "id": "89L45MziuGnA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(df_percentage_by_top_20['Date'], df_percentage_by_top_20['Percentage by Top 20%'], label='Percentage by Top 20%', color='blue')\n",
        "plt.axhline(y=average_percentage_by_top_20, color='r', linestyle='--', label=f'Average: {average_percentage_by_top_20:.2f}%')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('Percentage of Total Daily Sales Contributed by Top 20% Products')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(df_percentage_by_top_20['Date'][::int(len(df_percentage_by_top_20)/10)], rotation=45)  # Show only a subset of dates for readability\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zfGAFoqwuIxP"
      },
      "id": "zfGAFoqwuIxP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 그래프는 2022년 1월 1일부터 2023년 4월 4일까지 상위 20%의 제품이 전체 일별 매출에 차지하는 비율을 나타냅니다. 빨간색 점선은 해당 기간 동안의 평균 비율 (약 97.24)을 표시합니다."
      ],
      "metadata": {
        "id": "GaaA7dvKvoKp"
      },
      "id": "GaaA7dvKvoKp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "일별, 전체 매출의 약 97&는 전체 제품의 20%가 기여하고 있다."
      ],
      "metadata": {
        "id": "pTdmiLS3vjrz"
      },
      "id": "pTdmiLS3vjrz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 대분류별 기준(일별)"
      ],
      "metadata": {
        "id": "0RwILuFGuL0o"
      },
      "id": "0RwILuFGuL0o"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the average percentage of total daily sales contributed by the top 20% products for each 대분류\n",
        "average_percentage_by_top_20_per_category = {}\n",
        "\n",
        "# Get unique 대분류 categories\n",
        "unique_categories = df['대분류'].unique()\n",
        "\n",
        "# Loop through each 대분류 to calculate the required metrics\n",
        "for category in unique_categories:\n",
        "    # Filter the DataFrame to only include rows corresponding to the current 대분류\n",
        "    df_filtered = df[df['대분류'] == category]\n",
        "\n",
        "    # Drop non-sales related columns to focus on the sales data\n",
        "    sales_data_filtered = df_filtered.drop(columns=['ID', '제품', '대분류', '중분류', '소분류', '브랜드'])\n",
        "\n",
        "    # Transpose the DataFrame to have dates as rows and products as columns\n",
        "    sales_data_filtered_transposed = sales_data_filtered.transpose()\n",
        "\n",
        "    # Calculate the daily total sales for the filtered data\n",
        "    daily_total_sales_filtered = sales_data_filtered_transposed.sum(axis=1)\n",
        "\n",
        "    # Initialize a list to store the daily percentages for the current 대분류\n",
        "    daily_percentages = []\n",
        "\n",
        "    # Loop through each day to find the percentage of total daily sales contributed by the top 20% products\n",
        "    for date, total_sales in daily_total_sales_filtered.iteritems():\n",
        "        # Get the sorted daily sales for the date\n",
        "        sorted_daily_sales = sales_data_filtered_transposed.loc[date].sort_values(ascending=False)\n",
        "\n",
        "        # Calculate the number of top 20% products\n",
        "        num_top_20_products = int(len(sorted_daily_sales) * 0.2)\n",
        "\n",
        "        # Get the sales of the top 20% products and sum them up\n",
        "        top_20_sales_sum = sorted_daily_sales.head(num_top_20_products).sum()\n",
        "\n",
        "        # Calculate the percentage of total daily sales contributed by the top 20% products\n",
        "        percentage = (top_20_sales_sum / total_sales) * 100 if total_sales != 0 else 0\n",
        "\n",
        "        # Append the percentage to the list\n",
        "        daily_percentages.append(percentage)\n",
        "\n",
        "    # Calculate the average percentage for the current 대분류 and store it in the dictionary\n",
        "    average_percentage_by_top_20_per_category[category] = sum(daily_percentages) / len(daily_percentages)\n",
        "\n",
        "# Convert the dictionary to a DataFrame for better visualization\n",
        "df_average_percentage_by_top_20_per_category = pd.DataFrame(list(average_percentage_by_top_20_per_category.items()), columns=['대분류', 'Average Percentage by Top 20%'])\n",
        "\n",
        "df_average_percentage_by_top_20_per_category"
      ],
      "metadata": {
        "id": "DSkwtQBVuOiW"
      },
      "id": "DSkwtQBVuOiW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 대분류 기준(전체 기간)"
      ],
      "metadata": {
        "id": "zFlMoR5suQrP"
      },
      "id": "zFlMoR5suQrP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total sales for each product\n",
        "df['Total_Sales'] = df.iloc[:, 6:-1].sum(axis=1)  # Exclude the '상위 20% 브랜드' column if it exists\n",
        "\n",
        "# Initialize a dictionary to store the percentage of total sales contributed by the top 20% products for each 대분류\n",
        "percentage_by_top_20_per_category = {}\n",
        "\n",
        "# Calculate the 80th percentile of Total_Sales for each '대분류'\n",
        "top_20_thresholds = df.groupby('대분류')['Total_Sales'].quantile(0.8)\n",
        "\n",
        "# Loop through each 대분류 to calculate the required metrics\n",
        "for category, threshold in top_20_thresholds.iteritems():\n",
        "    # Filter the DataFrame to only include rows corresponding to the current 대분류\n",
        "    df_filtered = df[df['대분류'] == category]\n",
        "\n",
        "    # Calculate the total sales for the filtered DataFrame\n",
        "    total_sales_filtered = df_filtered['Total_Sales'].sum()\n",
        "\n",
        "    # Get the total sales of the top 20% products within the filtered DataFrame\n",
        "    top_20_sales_filtered = df_filtered[df_filtered['Total_Sales'] > threshold]['Total_Sales'].sum()\n",
        "\n",
        "    # Calculate the percentage of total sales contributed by the top 20% products\n",
        "    percentage = (top_20_sales_filtered / total_sales_filtered) * 100 if total_sales_filtered != 0 else 0\n",
        "\n",
        "    # Store the result in the dictionary\n",
        "    percentage_by_top_20_per_category[category] = percentage\n",
        "\n",
        "# Convert the dictionary to a DataFrame for better visualization\n",
        "df_percentage_by_top_20_per_category = pd.DataFrame(list(percentage_by_top_20_per_category.items()), columns=['대분류', 'Percentage by Top 20%'])\n",
        "\n",
        "# Drop the temporary 'Total_Sales' column\n",
        "df = df.drop(columns=['Total_Sales'])\n",
        "\n",
        "df_percentage_by_top_20_per_category"
      ],
      "metadata": {
        "id": "JXQN16o9uSLg"
      },
      "id": "JXQN16o9uSLg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 '대분류' 내에서 상위 20%에 속하는 제품을 식별하고, 그 정보를 '상위 20% 브랜드' 열에 표시합니다. 이를 통해 각 '대분류' 내에서 높은 판매량을 기록한 제품을 쉽게 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "ydh2g-u_uVyl"
      },
      "id": "ydh2g-u_uVyl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 기간 동안 각 대분류 내에서 상위 20%에 속하는 제품이 차지하는 매출 비율은 다음과 같습니다:  \n",
        "\n",
        "대분류 B002-C001-0001: 약 90.96%  \n",
        "대분류 B002-C001-0002: 약 92.80%  \n",
        "대분류 B002-C001-0003: 약 74.56%  \n",
        "대분류 B002-C001-0004: 약 69.10%  \n",
        "대분류 B002-C001-0005: 약 88.03%  \n",
        "\n",
        "평균 : 82.69%  \n",
        "약 80%의 매출을 차지한다고 판단할 수 있습니다."
      ],
      "metadata": {
        "id": "BctcvYVYuYMf"
      },
      "id": "BctcvYVYuYMf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 월별 판매량 분석"
      ],
      "metadata": {
        "id": "k7FZQ-IarBTM"
      },
      "id": "k7FZQ-IarBTM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"data/train.csv\")\n",
        "\n",
        "# '소분류' 마다 '제품'의 종류가 얼마나 있는지 확인\n",
        "\n",
        "# Count the unique '제품' within each '소분류'\n",
        "product_counts = data.groupby('소분류')['제품'].nunique()\n",
        "\n",
        "# Show the result\n",
        "product_counts"
      ],
      "metadata": {
        "id": "IF26Nmz-sza9"
      },
      "id": "IF26Nmz-sza9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(14,8))\n",
        "\n",
        "# Create bar plot\n",
        "sns.barplot(x=product_counts.index, y=product_counts.values, color='b')\n",
        "\n",
        "# Rotate x-axis labels for better visibility\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Set title and labels\n",
        "plt.title('Number of Unique Products per Subcategory')\n",
        "plt.xlabel('Subcategory')\n",
        "plt.ylabel('Number of Unique Products')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y2wXWf9gs6-r"
      },
      "id": "Y2wXWf9gs6-r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 월별 판매량을 확인\n",
        "\n",
        "# Extract the date columns\n",
        "date_columns = data.columns[6:]\n",
        "\n",
        "# Convert the column names to datetime\n",
        "dates = pd.to_datetime(date_columns)\n",
        "\n",
        "# Transpose the data and set the dates as the index\n",
        "sales_data = data[date_columns].T\n",
        "sales_data.index = dates\n",
        "\n",
        "# Resample the data to monthly frequency and sum the sales\n",
        "monthly_sales = sales_data.resample('M').sum()\n",
        "\n",
        "# Transpose the data back to the original form\n",
        "monthly_sales = monthly_sales.T\n",
        "monthly_sales = monthly_sales.iloc[:,:-1]\n",
        "\n",
        "# Show the first few rows\n",
        "monthly_sales.head()"
      ],
      "metadata": {
        "id": "TOK-zRTLtALg"
      },
      "id": "TOK-zRTLtALg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 월별 판매량 Top5는 계속해서 변하는가?"
      ],
      "metadata": {
        "id": "ksWLyewGtFue"
      },
      "id": "ksWLyewGtFue"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 제품별로 분석"
      ],
      "metadata": {
        "id": "5VgC_Sf2tJDV"
      },
      "id": "5VgC_Sf2tJDV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the '제품' column to the 'monthly_sales' dataframe\n",
        "monthly_sales['제품'] = data['제품']\n",
        "\n",
        "# For each month, sort the products by sales and select the top 5\n",
        "top_5_products_monthly = monthly_sales.set_index('제품').apply(lambda x: x.nlargest(5).index.tolist())\n",
        "\n",
        "# Transpose the result for better readability\n",
        "top_5_products_monthly = top_5_products_monthly.T\n",
        "\n",
        "# Show the result\n",
        "top_5_products_monthly"
      ],
      "metadata": {
        "id": "OuU_Wly3tD0D"
      },
      "id": "OuU_Wly3tD0D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 소분류별로 분석"
      ],
      "metadata": {
        "id": "KwvS06fztQTv"
      },
      "id": "KwvS06fztQTv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the '소분류' column to the 'monthly_sales' dataframe\n",
        "monthly_sales['소분류'] = data['소분류']\n",
        "\n",
        "# For each month, group by '소분류', sum the sales, sort by the sum, and select the top 5\n",
        "top_5_subcategories_monthly = monthly_sales.groupby('소분류').sum().apply(lambda x: x.nlargest(5).index.tolist())\n",
        "\n",
        "# Transpose the result for better readability\n",
        "top_5_subcategories_monthly = top_5_subcategories_monthly.T\n",
        "\n",
        "# Show the result\n",
        "top_5_subcategories_monthly"
      ],
      "metadata": {
        "id": "GgYzRi5PtHzc"
      },
      "id": "GgYzRi5PtHzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 0041, 0003, 0025, 0004는 Top 4에 꾸준하게 등장하고 있다.\n",
        "    \n",
        "    **제품 특성**을 확인해서 어떤 상품인지 확인했다.\n",
        "    \n",
        "    0041 : **`유아용 귀저귀`**\n",
        "    \n",
        "    0003 : **`영양제(건강식품)`**\n",
        "    \n",
        "    0025 : `**물티슈**`\n",
        "    \n",
        "    0004 : **`단백질보충제`**\n",
        "    \n",
        "- 0001**`(건강기능식품)`** : 2022년 12월 이후, 순위권에서 사라진다.\n",
        "    \n",
        "    January 2023: 7th\n",
        "    \n",
        "    February 2023: 6th\n",
        "    \n",
        "    March 2023: 6th\n",
        "    \n",
        "    → 꾸준하게 감소하고 있다.\n",
        "    \n",
        "- 0052**`(유아 식품)`** : 2022년 12월 이후, 순위권에 등장!\n",
        "    \n",
        "    January 2022: 8th\n",
        "    \n",
        "    February 2022: 8th\n",
        "    \n",
        "    March 2022: 7th\n",
        "    \n",
        "    April 2022: 6th\n",
        "    \n",
        "    May 2022: 7th\n",
        "    \n",
        "    June 2022: 7th\n",
        "    \n",
        "    July 2022: 6th\n",
        "    \n",
        "    August 2022: 6th\n",
        "    \n",
        "    September 2022: 6th\n",
        "    \n",
        "    October 2022: 6th\n",
        "    \n",
        "    November 2022: 7th\n",
        "    \n",
        "    December 2022: 5th"
      ],
      "metadata": {
        "id": "WB6yLq1xuyfb"
      },
      "id": "WB6yLq1xuyfb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분류 내에서 판매량 Top 5인 소분류를 확인"
      ],
      "metadata": {
        "id": "d78eCJjetTLP"
      },
      "id": "d78eCJjetTLP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the '대분류' column to the 'monthly_sales' dataframe\n",
        "monthly_sales['대분류'] = data['대분류']\n",
        "\n",
        "# For each '대분류', group by '소분류', sum the sales, sort by the sum, and select the top 5\n",
        "top_5_subcategories_in_major_category = monthly_sales.groupby(['대분류', '소분류']).sum().sum(axis=1).groupby(level=0).nlargest(5)\n",
        "\n",
        "# Show the result\n",
        "top_5_subcategories_in_major_category"
      ],
      "metadata": {
        "id": "R9Rgr3H7tZAo"
      },
      "id": "R9Rgr3H7tZAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 15개월 동안 꾸준하게 잘 팔리는 베스트 상품 4가지를 발견\n",
        "    \n",
        "    귀저귀 영양제 물티슈 단백질보충제"
      ],
      "metadata": {
        "id": "j7pSJjvbu2Hl"
      },
      "id": "j7pSJjvbu2Hl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시계열 데이터 패턴 추출"
      ],
      "metadata": {
        "id": "b1CuXdmerAev"
      },
      "id": "b1CuXdmerAev"
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('data/train.csv')"
      ],
      "metadata": {
        "id": "CvsmXiec0Q1N"
      },
      "id": "CvsmXiec0Q1N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose the dataframe so that dates are the index and products are the columns\n",
        "sales_data = df_raw.drop(['ID', '대분류', '중분류', '소분류', '브랜드'], axis=1).set_index('제품').transpose()\n",
        "\n",
        "# Convert the index to datetime\n",
        "sales_data.index = pd.to_datetime(sales_data.index)\n",
        "\n",
        "# Sum the sales data by month\n",
        "monthly_sales_data = sales_data.resample('M').sum()\n",
        "\n",
        "monthly_sales_data.transpose()\n",
        "df_month = monthly_sales_data.reset_index()\n",
        "df_month.rename(columns = {'index' : 'date'}, inplace = True)\n",
        "df_month.rename(columns = {'제품' : 'Index'}, inplace = True)"
      ],
      "metadata": {
        "id": "9RC7tPhh0-W8"
      },
      "id": "9RC7tPhh0-W8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# additive model: trend + cycle + seasonality + irregular factor\n",
        "\n",
        "timestamp = np.arange(len(df_month))\n",
        "\n",
        "trend_factor = timestamp*1.1\n",
        "\n",
        "cycle_factor = 10*np.sin(np.linspace(0, 3.14*2, 16))\n",
        "\n",
        "seasonal_factor = 7*np.sin(np.linspace(0, 3.14*8, 16))\n",
        "\n",
        "np.random.seed(2004)\n",
        "\n",
        "irregular_factor = 2*np.random.randn(len(df_month))\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'timeseries': trend_factor + cycle_factor + seasonal_factor + irregular_factor,\n",
        "\n",
        "                   'trend': trend_factor,\n",
        "\n",
        "                   'cycle': cycle_factor,\n",
        "\n",
        "                   'seasonal': seasonal_factor,\n",
        "\n",
        "                   'irregular': irregular_factor},\n",
        "\n",
        "                   index=df_month['date'])"
      ],
      "metadata": {
        "id": "HM45PbR61JKO"
      },
      "id": "HM45PbR61JKO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 6])\n",
        "\n",
        "df.timeseries.plot()\n",
        "\n",
        "plt.title('Time Series (Additive Model)', fontsize=16)\n",
        "\n",
        "plt.ylim(-12, 55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pqpEoiJO1Lbp"
      },
      "id": "pqpEoiJO1Lbp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Trend factor\n",
        "\n",
        "#timestamp = np.arange(len(dates))\n",
        "\n",
        "#trend_factor = timestamp*1.1\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 6])\n",
        "\n",
        "df.trend.plot()\n",
        "\n",
        "plt.title('Trend Factor', fontsize=16)\n",
        "\n",
        "plt.ylim(-12, 55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iTvdc0ye1Nj2"
      },
      "id": "iTvdc0ye1Nj2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Cycle factor\n",
        "\n",
        "#cycle_factor = 10*np.sin(np.linspace(0, 3.14*2, 16))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 6])\n",
        "\n",
        "df.cycle.plot()\n",
        "\n",
        "plt.title('Cycle Factor', fontsize=16)\n",
        "\n",
        "plt.ylim(-12, 55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "neOsIocp1PG6"
      },
      "id": "neOsIocp1PG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Seasonal factor\n",
        "\n",
        "#seasonal_factor = 7*np.sin(np.linspace(0, 3.14*8, 16))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 6])\n",
        "\n",
        "df.seasonal.plot()\n",
        "\n",
        "plt.title('Seasonal Factor', fontsize=16)\n",
        "\n",
        "plt.ylim(-12, 55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wNI4dJVi1QbG"
      },
      "id": "wNI4dJVi1QbG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Irregular/ Random factor\n",
        "\n",
        "#np.random.seed(2004)\n",
        "\n",
        "#irregular_factor = 2*np.random.randn(len(dates))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 6])\n",
        "\n",
        "df.irregular.plot()\n",
        "\n",
        "plt.title('Irregular Factor', fontsize=16)\n",
        "\n",
        "plt.ylim(-12, 55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNrEOo671S6r"
      },
      "id": "kNrEOo671S6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All in one: Time series = Trend factor + Cycle factor + Seasonal factor + Irregular factor\n",
        "\n",
        "\n",
        "from pylab import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "df.plot()\n",
        "\n",
        "plt.ylim(-12, 55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSK6KymV1UT-"
      },
      "id": "dSK6KymV1UT-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "56af1fd9",
      "metadata": {
        "id": "56af1fd9"
      },
      "source": [
        "# processing(파생변수 생성)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c35f8e",
      "metadata": {
        "id": "51c35f8e"
      },
      "source": [
        "### 상위 20% 브랜드 컬럼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac32f04c",
      "metadata": {
        "id": "ac32f04c"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('data/train.csv')\n",
        "\n",
        "# Calculate the total sales for each product\n",
        "df['Total_Sales'] = df.iloc[:, 6:].sum(axis=1)\n",
        "\n",
        "# Create a new column for \"Top 20% Brand\" and set default value to 'no'\n",
        "df['상위 20% 브랜드'] = 'no'\n",
        "\n",
        "# Calculate the 80th percentile of Total_Sales for each '대분류'\n",
        "top_20_thresholds = df.groupby('대분류')['Total_Sales'].quantile(0.8)\n",
        "\n",
        "# Set '상위 20% 브랜드' to 'yes' for products in the top 20% within their '대분류'\n",
        "for category, threshold in top_20_thresholds.iteritems():\n",
        "    df.loc[(df['대분류'] == category) & (df['Total_Sales'] > threshold), '상위 20% 브랜드'] = 'yes'\n",
        "\n",
        "# Drop the temporary 'Total_Sales' column\n",
        "df = df.drop(columns=['Total_Sales'])\n",
        "\n",
        "# Move the '상위 20% 브랜드' column to the right of the '브랜드' column\n",
        "cols = df.columns.tolist()\n",
        "cols.insert(cols.index('브랜드')+1, cols.pop(cols.index('상위 20% 브랜드')))\n",
        "df = df[cols]\n",
        "\n",
        "# Save the updated dataframe to a new CSV file again\n",
        "df.to_csv('data/train_TopBrand.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6005c3bb",
      "metadata": {
        "id": "6005c3bb"
      },
      "source": [
        "### 제품특성 컬럼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c90bb1",
      "metadata": {
        "id": "61c90bb1"
      },
      "outputs": [],
      "source": [
        "# Load the data (train_TopBrand와 product_info file load)\n",
        "train = pd.read_csv(\"data/train_TopBrand.csv\",index_col='ID')\n",
        "product = pd.read_csv(\"data/product_info.csv\")\n",
        "\n",
        "# 제품 컬럼 기준으로 데이터 정렬\n",
        "productt = product.sort_values(by='제품')\n",
        "dp = list(set(list(productt['제품'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9028de56",
      "metadata": {
        "id": "9028de56"
      },
      "outputs": [],
      "source": [
        "h = 0;j = 0\n",
        "hh = []\n",
        "while h <15890:\n",
        "    pro = train.iloc[h][0]\n",
        "\n",
        "    ## product 제품에 없는 train 제품은 '알 수 없음'으로 처리\n",
        "    if pro not in dp:\n",
        "        hh.append('알 수 없음')\n",
        "    ## 있는 제품은 product 파일 내 해당 제품의 제품설명으로 처리\n",
        "    else:\n",
        "        p = productt[productt['제품']==pro]\n",
        "        p = p.values\n",
        "        pa = list(p[0])\n",
        "        hh.append(pa[1])\n",
        "    h += 1\n",
        "\n",
        "## '제품특성'이라는 컬럼명으로 컬럼 추가\n",
        "train['제품특성'] = hh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a75e5df",
      "metadata": {
        "id": "1a75e5df"
      },
      "source": [
        "### 카테고리 컬럼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4348c73",
      "metadata": {
        "id": "c4348c73"
      },
      "outputs": [],
      "source": [
        "hh = []\n",
        "h = 0\n",
        "while h <15890:\n",
        "    ## 중분류 기준으로 '카테고리' 컬럼 추가\n",
        "    pro = train.iloc[h][2]\n",
        "\n",
        "    if pro == \"B002-C002-0001\":\n",
        "        hh.append('건강기능식품')\n",
        "    elif pro == \"B002-C002-0002\":\n",
        "        hh.append('탈취/방충/살충/제습/방향')\n",
        "    elif pro == \"B002-C002-0003\":\n",
        "        hh.append('주방/청소/세탁세제')\n",
        "    elif pro == \"B002-C002-0004\":\n",
        "        hh.append('욕실용품')\n",
        "    elif pro == \"B002-C002-0005\":\n",
        "        hh.append('제지/위생용품')\n",
        "    elif pro == \"B002-C002-0006\":\n",
        "        hh.append('헤어/바디/스킨/면도')\n",
        "    elif pro == \"B002-C002-0007\":\n",
        "        hh.append('특수헤어용품')\n",
        "    elif pro == \"B002-C002-0008\":\n",
        "        hh.append('유아 생활용품')\n",
        "    elif pro == \"B002-C002-0009\":\n",
        "        hh.append('유아 위생용품')\n",
        "    elif pro == \"B002-C002-0010\":\n",
        "        hh.append('뷰티용품')\n",
        "    else:\n",
        "        hh.append('유아식품')\n",
        "    h += 1\n",
        "\n",
        "train['카테고리'] = hh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662368ef",
      "metadata": {
        "id": "662368ef"
      },
      "outputs": [],
      "source": [
        "train.drop(['제품특성','중분류'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcdbd847",
      "metadata": {
        "id": "fcdbd847"
      },
      "outputs": [],
      "source": [
        "last_column = train.columns[-1]\n",
        "# 맨 뒤의 컬럼을 제외한 모든 컬럼을 리스트에 추가\n",
        "new_order = list(train.columns[:-1])\n",
        "# 5번째 위치에 맨 뒤의 컬럼 삽입\n",
        "new_order.insert(5, last_column)\n",
        "\n",
        "# 컬럼 순서 변경\n",
        "train = train[new_order]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52335b5",
      "metadata": {
        "id": "d52335b5"
      },
      "source": [
        "### 대량판매 컬럼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95dc5cf",
      "metadata": {
        "id": "b95dc5cf"
      },
      "outputs": [],
      "source": [
        "## EDA를 바탕으로 월별 꾸준히 많이 팔리는 제품을 선정함.\n",
        "\n",
        "good = ['0041','0003','0025','0004','0001','0052']\n",
        "### 기저귀, 유산균, 물티슈, 단백질, 건강기능식품, 아기식품\n",
        "\n",
        "h = 0\n",
        "hh = []\n",
        "while h < 15890:\n",
        "    line = train.iloc[h][2]\n",
        "    if line[10:] in good:\n",
        "        hh.append('yes')\n",
        "    else:\n",
        "        hh.append('no')\n",
        "    h += 1\n",
        "\n",
        "train['대량판매'] = hh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e27bf98",
      "metadata": {
        "id": "1e27bf98"
      },
      "outputs": [],
      "source": [
        "last_column = train.columns[-1]\n",
        "# 맨 뒤의 컬럼을 제외한 모든 컬럼을 리스트에 추가\n",
        "new_order = list(train.columns[:-1])\n",
        "# 6번째 위치에 맨 뒤의 컬럼 삽입\n",
        "new_order.insert(6, last_column)\n",
        "\n",
        "# 컬럼 순서 변경\n",
        "train = train[new_order]\n",
        "\n",
        "train.to_csv('trainTop_ctg_steady.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1139a514",
      "metadata": {
        "id": "1139a514"
      },
      "source": [
        "### 주기성 컬럼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d01ed2",
      "metadata": {
        "id": "62d01ed2"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('trainTop_ctg_steady.csv')\n",
        "\n",
        "##EDA를 바탕으로 각 카테고리별 주기성 확인 가능. 이를 토대로 period 컬럼 생성\n",
        "\n",
        "def period(x) :\n",
        "    if x in [\"건강기능식품\", \"유아 위생용품\", \"특수헤어용품\"] :\n",
        "        return \"steady\"\n",
        "    elif x in [\"유아 생활용품\", \"유아식품\", \"주방/청소/세탁세제\", \"헤어/바디/스킨/면도\"] :\n",
        "        return \"5개월\"\n",
        "    elif x == \"탈취/방충/살충/제습/방향\" :\n",
        "        return \"1년\"\n",
        "    elif x == \"욕실용품\" :\n",
        "        return \"4개월\"\n",
        "    elif x == \"제지/위생용품\" :\n",
        "        return \"3개월\"\n",
        "\n",
        "train['period'] = train['카테고리'].apply(period)\n",
        "\n",
        "last_column = train.columns[-1]\n",
        "# 맨 뒤의 컬럼을 제외한 모든 컬럼을 리스트에 추가\n",
        "new_order = list(train.columns[:-1])\n",
        "# 7번째 위치에 맨 뒤의 컬럼 삽입\n",
        "new_order.insert(7, last_column)\n",
        "\n",
        "# 컬럼 순서 변경\n",
        "train = train[new_order]\n",
        "\n",
        "train.to_csv('train_create.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "2tCxo-3kqa9W"
      },
      "id": "2tCxo-3kqa9W"
    },
    {
      "cell_type": "markdown",
      "id": "n8vBOVl4LXLR",
      "metadata": {
        "id": "n8vBOVl4LXLR"
      },
      "source": [
        "## 모델 파일(pt)를 저장할 경로"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "UDBcLrScKxVZ",
      "metadata": {
        "id": "UDBcLrScKxVZ"
      },
      "outputs": [],
      "source": [
        "# 모델의 종류, 변수의 종류가 변경될 때마다 수정해서 pt 파일을 저장할 경로 지정\n",
        "path = 'PT/LSTM_fc_'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fV0lSgZz_GDm",
      "metadata": {
        "id": "fV0lSgZz_GDm"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "AKS3uJfn-8sa",
      "metadata": {
        "id": "AKS3uJfn-8sa"
      },
      "outputs": [],
      "source": [
        "# 학습에 이용되는 변수 설정\n",
        "CFG = {\n",
        "    'TRAIN_WINDOW_SIZE':49, # 학습\n",
        "    'PREDICT_SIZE':21, # 예측\n",
        "    'EPOCHS':150, # 학습 횟수\n",
        "    'LEARNING_RATE':1e-4,\n",
        "    'BATCH_SIZE':256,\n",
        "    'SEED':41\n",
        "}\n",
        "\n",
        "encoding_x = 6 # 모델에 사용되는 명목변수의 종류\n",
        "split_rate = 0.2 # Validaion 데이터의 비율\n",
        "step_size = 14 # 학습 데이터 생성 간격"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lSpUgfWw_Id2",
      "metadata": {
        "id": "lSpUgfWw_Id2"
      },
      "source": [
        "## Fix Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "10RmfRmX--d3",
      "metadata": {
        "id": "10RmfRmX--d3"
      },
      "outputs": [],
      "source": [
        "# Seed 고정\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GB7o4MUg_LtG",
      "metadata": {
        "id": "GB7o4MUg_LtG"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "tyNCVrBk-_cW",
      "metadata": {
        "id": "tyNCVrBk-_cW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "deffebf7-872c-450f-d562-0b2a128dfeb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              대분류 상위 20% 브랜드 가격분류 대량판매  period day_week  2022-01-01  \\\n",
              "0  B002-C001-0002         no    중   no  steady      목요일           0   \n",
              "\n",
              "   2022-01-02  2022-01-03  2022-01-04  ...  2023-04-15  2023-04-16  \\\n",
              "0           0           0           0  ...           0           0   \n",
              "\n",
              "   2023-04-17  2023-04-18  2023-04-19  2023-04-20  2023-04-21  2023-04-22  \\\n",
              "0           0           0           0           0           0           0   \n",
              "\n",
              "   2023-04-23  2023-04-24  \n",
              "0           0           0  \n",
              "\n",
              "[1 rows x 485 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df3f340d-a1eb-4fa8-bc56-8db563f23e83\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>대분류</th>\n",
              "      <th>상위 20% 브랜드</th>\n",
              "      <th>가격분류</th>\n",
              "      <th>대량판매</th>\n",
              "      <th>period</th>\n",
              "      <th>day_week</th>\n",
              "      <th>2022-01-01</th>\n",
              "      <th>2022-01-02</th>\n",
              "      <th>2022-01-03</th>\n",
              "      <th>2022-01-04</th>\n",
              "      <th>...</th>\n",
              "      <th>2023-04-15</th>\n",
              "      <th>2023-04-16</th>\n",
              "      <th>2023-04-17</th>\n",
              "      <th>2023-04-18</th>\n",
              "      <th>2023-04-19</th>\n",
              "      <th>2023-04-20</th>\n",
              "      <th>2023-04-21</th>\n",
              "      <th>2023-04-22</th>\n",
              "      <th>2023-04-23</th>\n",
              "      <th>2023-04-24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B002-C001-0002</td>\n",
              "      <td>no</td>\n",
              "      <td>중</td>\n",
              "      <td>no</td>\n",
              "      <td>steady</td>\n",
              "      <td>목요일</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 485 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df3f340d-a1eb-4fa8-bc56-8db563f23e83')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df3f340d-a1eb-4fa8-bc56-8db563f23e83 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df3f340d-a1eb-4fa8-bc56-8db563f23e83');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# train 파일 불러오기\n",
        "# columns=['ID',\"제품\",\"대분류\",\"소분류\",\"브랜드\",\"상위 20% 브랜드\", \"카테고리\", \"대량판매\",\"trend\"])\n",
        "\n",
        "train_data = pd.read_csv('data/train_off4.csv').drop(columns=['ID',\"제품\",\"소분류\",\"브랜드\", \"카테고리\", \"판매가격\",'가격분류'])\n",
        "\n",
        "train_data.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PpEpp578_NV2",
      "metadata": {
        "id": "PpEpp578_NV2"
      },
      "source": [
        "## RSFA metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "MuogIDBmbAQj",
      "metadata": {
        "id": "MuogIDBmbAQj"
      },
      "outputs": [],
      "source": [
        "# 제품별 대분류를 저장하기 위한 dictionary 생성\n",
        "indexs_bigcat={}\n",
        "for bigcat in train_data['대분류'].unique():\n",
        "    indexs_bigcat[bigcat] = list(train_data.loc[train_data['대분류']==bigcat].index)\n",
        "\n",
        "indexs_bigcat.keys()\n",
        "\n",
        "# Validation 데이터에서 RSFA Score 계산을 위한 함수 구현\n",
        "def PSFA(pred, target):\n",
        "    PSFA = 1\n",
        "    epsilon = 1e-10  # 매우 작은 값(분모가 0이 되는 것을 방지)\n",
        "\n",
        "    for cat in indexs_bigcat.keys():\n",
        "        ids = indexs_bigcat[cat]\n",
        "        for day in range(21):\n",
        "            total_sell = np.sum(target[ids, day]) # day별 총 판매량\n",
        "            pred_values = pred[ids, day] # day별 예측 판매량\n",
        "            target_values = target[ids, day] # day별 실제 판매량\n",
        "\n",
        "            # 실제 판매와 예측 판매가 같은 경우 오차가 없는 것으로 간주\n",
        "            denominator = np.maximum(target_values, pred_values) + epsilon\n",
        "            diffs = np.abs(target_values - pred_values) / denominator\n",
        "\n",
        "            if total_sell != 0:\n",
        "                sell_weights = target_values / total_sell  # Item별 day 총 판매량 내 비중\n",
        "            else:\n",
        "                sell_weights = np.ones_like(target_values) / len(ids)  # 1 / len(ids)로 대체\n",
        "\n",
        "            if not np.isnan(diffs).any():  # diffs에 NaN이 없는 경우에만 PSFA 값 업데이트\n",
        "                PSFA -= np.sum(diffs * sell_weights) / (21 * 5)\n",
        "\n",
        "    return PSFA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "HgNFkh93oRVN",
      "metadata": {
        "id": "HgNFkh93oRVN"
      },
      "outputs": [],
      "source": [
        "# 사용자 Loss Fucntion 구현\n",
        "## 일일 판매 비중을 계산하여 판매량이 높은 제품일수록 정확하게 예측하도록 유도\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # 일일 판매량의 절대 차이\n",
        "        abs_diff = torch.abs(y_pred - y_true)\n",
        "\n",
        "        # max(실제값, 예측값)\n",
        "        max_values = torch.max(y_true, y_pred)\n",
        "\n",
        "        # 특정 일의 모든 제품 판매량 합계\n",
        "        sum_values = torch.sum(y_true, dim=1, keepdim=True)\n",
        "\n",
        "        # 제품의 일일 판매량 실제값의 비중\n",
        "        sales_weight = y_true / (sum_values + 1e-10)\n",
        "\n",
        "        # 최종 손실 계산\n",
        "        loss = torch.mean(sales_weight * (abs_diff / (max_values + 1e-10)))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bqn_zwU7_Qce",
      "metadata": {
        "id": "bqn_zwU7_Qce"
      },
      "source": [
        "## Min-Max Scaling & Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CZU8LXGDT6o7",
      "metadata": {
        "id": "CZU8LXGDT6o7"
      },
      "source": [
        "### Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3Ye_vEc7P7U0",
      "metadata": {
        "id": "3Ye_vEc7P7U0"
      },
      "outputs": [],
      "source": [
        "# Apply Label Encoding(\"대분류\",\"소분류\",\"브랜드\",\"상위 20% 브랜드\", \"카테고리\", \"대량판매\", \"trend\")\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "le3 = LabelEncoder()\n",
        "le4 = LabelEncoder()\n",
        "le5 = LabelEncoder()\n",
        "le6 = LabelEncoder()\n",
        "train_data['대분류'] = le1.fit_transform(train_data['대분류'])\n",
        "# train_data['소분류'] = le2.fit_transform(train_data['소분류'])\n",
        "# train_data['쇼핑몰'] = le3.fit_transform(train_data['쇼핑몰'])\n",
        "train_data['상위 20% 브랜드'] = le4.fit_transform(train_data['상위 20% 브랜드'])\n",
        "train_data['가격분류'] = le5.fit_transform(train_data['가격분류'])\n",
        "train_data['대량판매'] = le6.fit_transform(train_data['대량판매'])\n",
        "train_data['period'] = le6.fit_transform(train_data['period'])\n",
        "train_data['day_week'] = le6.fit_transform(train_data['day_week'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X8L36GeIT1aE",
      "metadata": {
        "id": "X8L36GeIT1aE"
      },
      "source": [
        "### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b85832b1-9f31-41e4-b934-cc260e5b3b1d",
      "metadata": {
        "id": "b85832b1-9f31-41e4-b934-cc260e5b3b1d"
      },
      "outputs": [],
      "source": [
        "# 먼저 2D NumPy 배열로 변환\n",
        "data_np = train_data.iloc[:, encoding_x:].values\n",
        "\n",
        "# 최대 및 최소값 계산\n",
        "max_vals = np.max(data_np, axis=1)\n",
        "min_vals = np.min(data_np, axis=1)\n",
        "\n",
        "# 분모가 0이되는 경우를 처리하려면 np.where를 사용하여 조건적으로 배열을 생성\n",
        "denominator = np.where(max_vals == min_vals, 1, max_vals - min_vals)\n",
        "\n",
        "# min-max scaling 수행\n",
        "scaled_data = (data_np - min_vals[:, None]) / denominator[:, None]\n",
        "\n",
        "# 결과를 DataFrame에 다시 저장\n",
        "train_data.iloc[:, encoding_x:] = scaled_data\n",
        "\n",
        "# 딕셔너리 생성\n",
        "scale_max_dict = dict(enumerate(max_vals))\n",
        "scale_min_dict = dict(enumerate(min_vals))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bg7S9U4R_S9n",
      "metadata": {
        "id": "bg7S9U4R_S9n"
      },
      "source": [
        "## Sliding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dY0ydw7_adw",
      "metadata": {
        "id": "0dY0ydw7_adw"
      },
      "source": [
        "### Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6Qc7K8nQIB3m",
      "metadata": {
        "id": "6Qc7K8nQIB3m"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def make_train_data(data, encoding_x = encoding_x, series = 1, train_size = CFG['TRAIN_WINDOW_SIZE'], predict_size = CFG['PREDICT_SIZE'], step_size=step_size):\n",
        "    num_rows = len(data)\n",
        "    window_size = train_size + predict_size\n",
        "    adjusted_size = ((len(data.columns) - window_size) // step_size) + 1\n",
        "\n",
        "    input_data = np.empty((num_rows * adjusted_size, train_size, series))\n",
        "    encoding_data = np.empty((num_rows * adjusted_size, train_size, encoding_x))\n",
        "    target_data = np.empty((num_rows * adjusted_size, predict_size))\n",
        "    indices_data = np.empty((num_rows * adjusted_size), dtype=int)\n",
        "\n",
        "    for i in tqdm(range(num_rows)):\n",
        "        encode_info = np.array(data.iloc[i, :encoding_x])\n",
        "        sales_data = np.array(data.iloc[i, encoding_x:])\n",
        "\n",
        "        for j in range(0, len(sales_data) - window_size + 1 - (window_size % step_size), step_size):\n",
        "            window = sales_data[j: j + window_size]\n",
        "            temp_data = window[:train_size]\n",
        "            index = i * adjusted_size + j // step_size\n",
        "            input_data[index] = temp_data.reshape(-1, 1)\n",
        "            encoding_data[index] = np.tile(encode_info, (train_size, 1))\n",
        "            target_data[index] = window[train_size:]\n",
        "            indices_data[index] = i\n",
        "\n",
        "    return input_data, encoding_data, target_data, indices_data\n",
        "\n",
        "def make_predict_data(data, encoding_x=encoding_x, series=1, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
        "    num_rows = len(data)\n",
        "    indices_data = np.empty(num_rows, dtype=int)\n",
        "    input_data = np.empty((num_rows, train_size, series))\n",
        "    encoding_data = np.empty((num_rows, train_size, encoding_x))\n",
        "\n",
        "    for i in tqdm(range(num_rows)):\n",
        "        encode_info = np.array(data.iloc[i, :encoding_x])\n",
        "        sales_data = np.array(data.iloc[i, encoding_x:])\n",
        "        indices_data[i] = i\n",
        "\n",
        "        window = sales_data[-train_size:]\n",
        "        temp_data = window.reshape(-1, 1)\n",
        "        input_data[i] = temp_data\n",
        "        encoding_data[i] = np.tile(encode_info, (train_size, 1))\n",
        "\n",
        "    return input_data, encoding_data, indices_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hPf5BWAx_cDW",
      "metadata": {
        "id": "hPf5BWAx_cDW"
      },
      "source": [
        "### Run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "LRPBBIRSezru",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRPBBIRSezru",
        "outputId": "1c0794c3-7891-4590-c9f4-3b73d8804ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28894/28894 [01:00<00:00, 479.09it/s]\n",
            "100%|██████████| 28894/28894 [00:51<00:00, 560.60it/s]\n"
          ]
        }
      ],
      "source": [
        "# 학습 데이터 및 평가 데이터 생성\n",
        "train_input, train_encoding, train_target, train_indices = make_train_data(train_data)\n",
        "test_input, test_encoding, test_indices = make_predict_data(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SaeadFr-_oZA",
      "metadata": {
        "id": "SaeadFr-_oZA"
      },
      "source": [
        "## Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "EQPMvLzXNOtP",
      "metadata": {
        "id": "EQPMvLzXNOtP"
      },
      "outputs": [],
      "source": [
        "# Train / Validation Split\n",
        "data_len = len(train_input)\n",
        "split_idx = int(data_len * (1 - split_rate))\n",
        "\n",
        "val_input, train_input = train_input[split_idx:], train_input[:split_idx]\n",
        "val_encoding, train_encoding = train_encoding[split_idx:], train_encoding[:split_idx]\n",
        "val_target, train_target = train_target[split_idx:], train_target[:split_idx]\n",
        "val_indices, train_indices = train_indices[split_idx:], train_indices[:split_idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.shape, val_input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4anpP7UZmm2",
        "outputId": "936d265b-66fc-4171-d4ed-dbe0d7d975f1"
      },
      "id": "z4anpP7UZmm2",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((693456, 49, 1), (173364, 49, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoding.shape, val_encoding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwpeoMqzZsIl",
        "outputId": "e4279e72-f728-4f50-affe-e7af79a3ed2b"
      },
      "id": "lwpeoMqzZsIl",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((693456, 49, 6), (173364, 49, 6))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_target.shape, val_target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLhbaqXRZwpU",
        "outputId": "0dace005-0f70-4057-ff95-c4d82efca130"
      },
      "id": "SLhbaqXRZwpU",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((693456, 21), (173364, 21))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices.shape, val_indices.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38-52lf9ZwgE",
        "outputId": "2c086e15-76ce-435f-8580-58e03d2ba1b4"
      },
      "id": "38-52lf9ZwgE",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((693456,), (173364,))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_pcuWr-o_pdv",
      "metadata": {
        "id": "_pcuWr-o_pdv"
      },
      "source": [
        "## CustomDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vOOa-DXC_w22",
      "metadata": {
        "id": "vOOa-DXC_w22"
      },
      "source": [
        "### Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9yoinQRLndU6",
      "metadata": {
        "id": "9yoinQRLndU6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 학습을 위한 CustomDataset 클래스\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_input, data_encoding, data_target, indices):\n",
        "        self.data_input = data_input  # 데이터셋의 입력 특성\n",
        "        self.data_target = data_target\n",
        "        self.data_encoding = data_encoding\n",
        "        self.indices = indices  # 원본 데이터에서의 행 인덱스\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_input)  # 데이터셋의 크기를 반환\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_input = self.data_input[idx].astype(np.float32)  # 주어진 인덱스에 대한 입력 특성을 가져오고 float32로 형변환\n",
        "        x_encoding = self.data_encoding[idx].astype(np.float32)  # 주어진 인덱스에 대한 입력 특성을 가져오고 float32로 형변환\n",
        "        original_idx = self.indices[idx]  # 샘플에 대한 원래 인덱스를 가져옴\n",
        "\n",
        "        # data_target이 None이 아니라면 입력 특성과 목표 값을 모두 반환\n",
        "        if self.data_target is not None:\n",
        "            y = self.data_target[idx].astype(np.float32)  # 주어진 인덱스에 대한 목표 값을 가져오고 float32로 형변환\n",
        "            return x_input, x_encoding, y, original_idx  # 입력 특성, 목표 값, 원래 인덱스를 반환\n",
        "        else:\n",
        "            return x_input, x_encoding, original_idx  # 목표 값이 없는 경우 입력 특성과 원래 인덱스만 반환\n",
        "\n",
        "# 테스트를 위한 CustomDataset 클래스\n",
        "class CustomDataset_Test(Dataset):\n",
        "    def __init__(self, x_input, x_encoding, Y):\n",
        "        self.x_input = x_input  # 데이터셋의 입력 특\n",
        "        self.x_encoding = x_encoding  # 데이터셋의 입력 특성\n",
        "        self.Y = Y  # 데이터셋의 목표 값 (None일 수 있음)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 목표 값이 있는 경우, 입력 특성, 목표 값, 인덱스를 반환\n",
        "        if self.Y is not None:\n",
        "            return torch.Tensor(self.x_input[index]), torch.Tensor(self.x_encoding[index]), torch.Tensor(self.Y[index]), index  # 인덱스 추가하여 추적\n",
        "        # 목표 값이 없는 경우, 입력 특성과 인덱스만 반환\n",
        "        return torch.Tensor(self.x_input[index]), torch.Tensor(self.x_encoding[index]), index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)  # 데이터셋의 크기를 반환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "emlEOON2_tOf",
      "metadata": {
        "id": "emlEOON2_tOf"
      },
      "source": [
        "### Run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Tarpe1sa_vam",
      "metadata": {
        "id": "Tarpe1sa_vam"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_input,train_encoding, train_target, train_indices)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataset = CustomDataset(val_input, val_encoding, val_target, val_indices)\n",
        "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KnCZG3vOWj-2",
      "metadata": {
        "id": "KnCZG3vOWj-2"
      },
      "source": [
        "## EarlyStoping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "55rHG9wyomvL",
      "metadata": {
        "id": "55rHG9wyomvL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0.0002, path=path):\n",
        "        self.patience = patience  # 얼마나 많은 epoch 동안 개선이 없을 경우 학습을 멈출지 설정\n",
        "        self.verbose = verbose  # 상세한 로그 출력 여부\n",
        "        self.counter = 0  # 개선이 없는 epoch 횟수 카운터\n",
        "        self.best_score = None  # 현재까지의 최고 점수\n",
        "        self.early_stop = False  # Early stopping 여부\n",
        "        self.val_loss_min = np.Inf  # 이전의 최소 검증 손실값\n",
        "        self.delta = delta  # 개선이 되었다고 판단할 최소 변화량\n",
        "        self.path = path  # 모델이 저장될 경로\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss  # 검증 손실을 점수로 변환 (손실은 작을수록 좋으므로 음수로 변환)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:  # 현재 점수가 이전 최고 점수보다 높지 않은 경우\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'조기 종료 카운터: {self.counter} / {self.patience}')\n",
        "            if self.counter >= self.patience:  # 카운터가 patience에 도달하면 조기 종료\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score  # 최고 점수 업데이트\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0  # 카운터 초기화\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''검증 손실이 감소하면 모델을 저장합니다.'''\n",
        "        if self.verbose:\n",
        "            print(f'검증 손실이 감소했습니다. ({self.val_loss_min:.6f} --> {val_loss:.6f}).  모델 저장 ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss  # 최소 검증 손실 업데이트\n",
        "\n",
        "    def get_val_loss_min(self):\n",
        "        return self.val_loss_min  # 현재까지의 최소 검증 손실값을 반환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tEhBAfBt_1KI",
      "metadata": {
        "id": "tEhBAfBt_1KI"
      },
      "source": [
        "## Model 구조 생성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, encoding_dim = encoding_x, series_dim=1, hidden_size=256, output_size=CFG['PREDICT_SIZE'], num_layers=2):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.hidden_size = hidden_size  # 은닉층의 노드 수\n",
        "\n",
        "        # LSTM 층 설정\n",
        "        self.lstm = nn.LSTM(input_size = encoding_x + series_dim, hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "        # 완전 연결층 (Fully Connected Layer)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size//2),\n",
        "            nn.BatchNorm1d(hidden_size//2),  # 선형 층 이후에 배치 정규화 추가\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(hidden_size//2, output_size)\n",
        "        )\n",
        "\n",
        "        # 활성화 함수로 ReLU 사용\n",
        "        self.actv = nn.ReLU()\n",
        "\n",
        "    def forward(self, series_input, encoding_input):\n",
        "        # Combining series_input and encoding input\n",
        "        combined_input = torch.cat((series_input, encoding_input), dim=2)\n",
        "\n",
        "        batch_size = combined_input.size(0)\n",
        "        # 초기 은닉 상태와 셀 상태 설정\n",
        "        hidden = self.init_hidden(batch_size, combined_input.device)\n",
        "\n",
        "        # LSTM 층을 통과\n",
        "        lstm_out, hidden = self.lstm(combined_input, hidden)\n",
        "\n",
        "        # 마지막 출력 시퀀스만 사용\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "\n",
        "        # 완전 연결층을 통과\n",
        "        output = self.actv(self.fc(last_output))\n",
        "\n",
        "        return output.squeeze(1)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        # 은닉 상태와 셀 상태 초기화\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
        "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
        "        return x"
      ],
      "metadata": {
        "id": "l6dF6COvMTs-"
      },
      "id": "l6dF6COvMTs-",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "DsyjoCT8_70Z",
      "metadata": {
        "id": "DsyjoCT8_70Z"
      },
      "source": [
        "### Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "TYGOlcQexiMC",
      "metadata": {
        "id": "TYGOlcQexiMC"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_loader, val_loader, device):\n",
        "    # 모델을 지정한 디바이스로 옮김\n",
        "    model.to(device)\n",
        "    best_psfa = 0  # 최고 PSFA 점수를 저장할 변수\n",
        "    best_loss = 99999  # 최저 손실값을 저장할 변수\n",
        "\n",
        "    # 사용자 정의 손실 함수 초기화\n",
        "    criterion = CustomLoss().to(device)\n",
        "\n",
        "    # CosineAnnealingLR 스케줄러 초기화\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
        "\n",
        "    # Epoch에 따라 Train과 Valdiation을 수행\n",
        "    for epoch in range(1, CFG['EPOCHS']+1):\n",
        "        model.train()  # 모델을 훈련 모드로 설정\n",
        "        train_loss = []\n",
        "\n",
        "        # 훈련 데이터로 모델 훈련\n",
        "        for data_input, data_encoding, data_target, indices in tqdm(iter(train_loader)):\n",
        "            data_input = data_input.to(device)\n",
        "            data_encoding = data_encoding.to(device)\n",
        "            data_target = data_target.to(device)\n",
        "\n",
        "            # 기울기 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 순방향 계산\n",
        "            output = model(data_input, data_encoding)\n",
        "            loss = criterion(output, data_target)\n",
        "\n",
        "            # 역방향 계산 및 파라미터 업데이트\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "        # 검증 데이터로 모델 검증 및 로그 출력\n",
        "        val_loss, psfa_score = validation(model, val_loader, criterion, device)\n",
        "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] PSFA Score : [{psfa_score:.5f}]')\n",
        "\n",
        "        # 손실값이 최소일 때 모델 저장\n",
        "        if best_loss > val_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_loss = model\n",
        "            torch.save(model.state_dict(), path + 'Model(best)_Loss.pt')\n",
        "\n",
        "        # PSFA 점수가 최대일 때 모델 저장\n",
        "        if best_psfa < psfa_score:\n",
        "            best_psfa = psfa_score\n",
        "            best_model_psfa = model\n",
        "            torch.save(model.state_dict(), path + 'Model(best)_PSFA.pt')\n",
        "\n",
        "        # 조기 종료 로직 실행\n",
        "        early_stopping(val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            print(f\"Best saved validation loss: {early_stopping.get_val_loss_min():.6f}\")\n",
        "            break\n",
        "\n",
        "        # 스케줄러 갱신\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_loss, best_psfa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fxVH0qL_-j4",
      "metadata": {
        "id": "6fxVH0qL_-j4"
      },
      "source": [
        "### Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "yz0dkmJaxjcb",
      "metadata": {
        "id": "yz0dkmJaxjcb"
      },
      "outputs": [],
      "source": [
        "def validation(model, val_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    val_loss = []  # 검증 손실을 저장할 리스트\n",
        "    pred_list = []  # 예측 값을 저장할 리스트\n",
        "    target_list = []  # 실제 타겟 값을 저장할 리스트\n",
        "    indices = []  # 배치의 인덱스를 저장할 리스트\n",
        "\n",
        "    # 기울기 계산을 하지 않는 블록\n",
        "    with torch.no_grad():\n",
        "        # 검증 데이터로 모델 평가\n",
        "        for data_input, data_encoding, data_target, i in tqdm(iter(val_loader)):\n",
        "            data_input = data_input.to(device)\n",
        "            data_encoding = data_encoding.to(device)\n",
        "            data_target = data_target.to(device)\n",
        "            output = model(data_input, data_encoding)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(output, data_target)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            # 예측값과 타겟을 CPU로 이동 후 NumPy 배열로 변환\n",
        "            output = output.cpu().numpy()\n",
        "            data_target = data_target.cpu().numpy()\n",
        "\n",
        "            # 예측값과 타겟을 리스트에 저장\n",
        "            pred_list.extend(output)\n",
        "            target_list.extend(data_target)\n",
        "            indices.extend(i.cpu().numpy())\n",
        "\n",
        "    # 예측값과 타겟을 NumPy 배열로 변환\n",
        "    pred_array = np.array(pred_list)\n",
        "    target_array = np.array(target_list)\n",
        "\n",
        "    # 각 예측값과 타겟을 스케일링\n",
        "    for i, idx in enumerate(indices):\n",
        "        pred_array[i, :] = pred_array[i, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
        "        target_array[i, :] = target_array[i, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
        "\n",
        "    # 예측값과 타겟을 반올림\n",
        "    pred_array = np.round(pred_array, 0).astype(int)\n",
        "    target_array = np.round(target_array, 0).astype(int)\n",
        "\n",
        "    # PSFA 점수 계산\n",
        "    psfa_score = PSFA(pred_array, target_array)\n",
        "\n",
        "    return np.mean(val_loss), psfa_score  # 평균 검증 손실과 PSFA 점수 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y4IzS8KLABTu",
      "metadata": {
        "id": "y4IzS8KLABTu"
      },
      "source": [
        "### Train 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "DI1a7DiSZkvA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI1a7DiSZkvA",
        "outputId": "b8d6a8da-ab5c-4e43-8b06-d3cc16e32211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.75it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 105.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [1] Train Loss : [0.01685] Val Loss : [0.01427] PSFA Score : [0.53074]\n",
            "검증 손실이 감소했습니다. (inf --> 0.014271).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:39<00:00, 68.15it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [2] Train Loss : [0.01435] Val Loss : [0.01404] PSFA Score : [0.53236]\n",
            "검증 손실이 감소했습니다. (0.014271 --> 0.014039).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 69.48it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 127.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [3] Train Loss : [0.01411] Val Loss : [0.01393] PSFA Score : [0.53379]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:40<00:00, 67.63it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 115.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [4] Train Loss : [0.01410] Val Loss : [0.01388] PSFA Score : [0.53623]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:39<00:00, 69.42it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 136.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [5] Train Loss : [0.01414] Val Loss : [0.01376] PSFA Score : [0.53751]\n",
            "검증 손실이 감소했습니다. (0.014039 --> 0.013762).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:39<00:00, 68.89it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 110.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [6] Train Loss : [0.01384] Val Loss : [0.01374] PSFA Score : [0.53639]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 69.95it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 125.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [7] Train Loss : [0.01377] Val Loss : [0.01412] PSFA Score : [0.53828]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:39<00:00, 69.32it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 137.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [8] Train Loss : [0.01372] Val Loss : [0.01365] PSFA Score : [0.53753]\n",
            "조기 종료 카운터: 3 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 69.99it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 102.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [9] Train Loss : [0.01369] Val Loss : [0.01426] PSFA Score : [0.53398]\n",
            "조기 종료 카운터: 4 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 69.57it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 126.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [10] Train Loss : [0.01362] Val Loss : [0.01351] PSFA Score : [0.54215]\n",
            "검증 손실이 감소했습니다. (0.013762 --> 0.013513).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:39<00:00, 69.36it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 137.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [11] Train Loss : [0.01367] Val Loss : [0.01336] PSFA Score : [0.54535]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.36it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 114.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [12] Train Loss : [0.01363] Val Loss : [0.01345] PSFA Score : [0.54694]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 71.21it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 121.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [13] Train Loss : [0.01360] Val Loss : [0.01337] PSFA Score : [0.54258]\n",
            "조기 종료 카운터: 3 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.51it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 137.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [14] Train Loss : [0.01358] Val Loss : [0.01348] PSFA Score : [0.53793]\n",
            "조기 종료 카운터: 4 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.99it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 113.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [15] Train Loss : [0.01358] Val Loss : [0.01351] PSFA Score : [0.53476]\n",
            "조기 종료 카운터: 5 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.41it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 126.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [16] Train Loss : [0.01388] Val Loss : [0.01345] PSFA Score : [0.54686]\n",
            "조기 종료 카운터: 6 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.56it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 143.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [17] Train Loss : [0.01366] Val Loss : [0.01317] PSFA Score : [0.54613]\n",
            "검증 손실이 감소했습니다. (0.013513 --> 0.013169).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.68it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 133.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [18] Train Loss : [0.01343] Val Loss : [0.01316] PSFA Score : [0.54587]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.83it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 109.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [19] Train Loss : [0.01334] Val Loss : [0.01335] PSFA Score : [0.55183]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.94it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 139.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [20] Train Loss : [0.01329] Val Loss : [0.01343] PSFA Score : [0.54352]\n",
            "조기 종료 카운터: 3 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.70it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 138.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [21] Train Loss : [0.01325] Val Loss : [0.01307] PSFA Score : [0.54543]\n",
            "조기 종료 카운터: 4 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.80it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 107.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [22] Train Loss : [0.01322] Val Loss : [0.01303] PSFA Score : [0.55108]\n",
            "조기 종료 카운터: 5 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.60it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 142.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [23] Train Loss : [0.01319] Val Loss : [0.01303] PSFA Score : [0.55359]\n",
            "조기 종료 카운터: 6 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.63it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [24] Train Loss : [0.01317] Val Loss : [0.01310] PSFA Score : [0.55360]\n",
            "조기 종료 카운터: 7 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.43it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 108.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [25] Train Loss : [0.01315] Val Loss : [0.01295] PSFA Score : [0.54511]\n",
            "검증 손실이 감소했습니다. (0.013169 --> 0.012947).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.71it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 137.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [26] Train Loss : [0.01313] Val Loss : [0.01305] PSFA Score : [0.54372]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.91it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [27] Train Loss : [0.01311] Val Loss : [0.01309] PSFA Score : [0.54494]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.14it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 105.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [28] Train Loss : [0.01308] Val Loss : [0.01307] PSFA Score : [0.56017]\n",
            "조기 종료 카운터: 3 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.76it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 131.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [29] Train Loss : [0.01304] Val Loss : [0.01329] PSFA Score : [0.54453]\n",
            "조기 종료 카운터: 4 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:45<00:00, 59.60it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 109.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [30] Train Loss : [0.01298] Val Loss : [0.01284] PSFA Score : [0.55996]\n",
            "조기 종료 카운터: 5 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.32it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [31] Train Loss : [0.01292] Val Loss : [0.01280] PSFA Score : [0.55586]\n",
            "조기 종료 카운터: 6 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.88it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 130.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [32] Train Loss : [0.01287] Val Loss : [0.01343] PSFA Score : [0.54468]\n",
            "조기 종료 카운터: 7 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.30it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 112.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [33] Train Loss : [0.01282] Val Loss : [0.01290] PSFA Score : [0.55006]\n",
            "조기 종료 카운터: 8 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.66it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [34] Train Loss : [0.01279] Val Loss : [0.01299] PSFA Score : [0.56406]\n",
            "조기 종료 카운터: 9 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.60it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 133.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [35] Train Loss : [0.01275] Val Loss : [0.01286] PSFA Score : [0.56569]\n",
            "조기 종료 카운터: 10 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.75it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 106.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [36] Train Loss : [0.01272] Val Loss : [0.01273] PSFA Score : [0.56363]\n",
            "검증 손실이 감소했습니다. (0.012947 --> 0.012726).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.79it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 142.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [37] Train Loss : [0.01271] Val Loss : [0.01354] PSFA Score : [0.54462]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.46it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 131.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [38] Train Loss : [0.01269] Val Loss : [0.01251] PSFA Score : [0.55451]\n",
            "검증 손실이 감소했습니다. (0.012726 --> 0.012508).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.49it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 108.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [39] Train Loss : [0.01267] Val Loss : [0.01283] PSFA Score : [0.56874]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.67it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 139.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [40] Train Loss : [0.01266] Val Loss : [0.01245] PSFA Score : [0.56267]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.44it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 133.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [41] Train Loss : [0.01264] Val Loss : [0.01240] PSFA Score : [0.56355]\n",
            "조기 종료 카운터: 3 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 71.26it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 107.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [42] Train Loss : [0.01262] Val Loss : [0.01243] PSFA Score : [0.56224]\n",
            "조기 종료 카운터: 4 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.33it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [43] Train Loss : [0.01261] Val Loss : [0.01238] PSFA Score : [0.55783]\n",
            "조기 종료 카운터: 5 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.66it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [44] Train Loss : [0.01260] Val Loss : [0.01353] PSFA Score : [0.54684]\n",
            "조기 종료 카운터: 6 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 71.19it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 109.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [45] Train Loss : [0.01259] Val Loss : [0.01252] PSFA Score : [0.56853]\n",
            "조기 종료 카운터: 7 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.45it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 137.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [46] Train Loss : [0.01259] Val Loss : [0.01297] PSFA Score : [0.55326]\n",
            "조기 종료 카운터: 8 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.43it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [47] Train Loss : [0.01258] Val Loss : [0.01231] PSFA Score : [0.56344]\n",
            "조기 종료 카운터: 9 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.37it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 109.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [48] Train Loss : [0.01257] Val Loss : [0.01228] PSFA Score : [0.56390]\n",
            "검증 손실이 감소했습니다. (0.012508 --> 0.012279).  모델 저장 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.59it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [49] Train Loss : [0.01256] Val Loss : [0.01241] PSFA Score : [0.55922]\n",
            "조기 종료 카운터: 1 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.51it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [50] Train Loss : [0.01255] Val Loss : [0.01227] PSFA Score : [0.56379]\n",
            "조기 종료 카운터: 2 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:37<00:00, 71.52it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 108.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [51] Train Loss : [0.01254] Val Loss : [0.01252] PSFA Score : [0.56794]\n",
            "조기 종료 카운터: 3 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.26it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [52] Train Loss : [0.01254] Val Loss : [0.01238] PSFA Score : [0.56700]\n",
            "조기 종료 카운터: 4 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.73it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 138.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [53] Train Loss : [0.01253] Val Loss : [0.01263] PSFA Score : [0.57110]\n",
            "조기 종료 카운터: 5 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.90it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 109.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [54] Train Loss : [0.01253] Val Loss : [0.01286] PSFA Score : [0.55676]\n",
            "조기 종료 카운터: 6 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.21it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 142.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [55] Train Loss : [0.01252] Val Loss : [0.01256] PSFA Score : [0.56266]\n",
            "조기 종료 카운터: 7 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.50it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 141.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [56] Train Loss : [0.01252] Val Loss : [0.01256] PSFA Score : [0.55888]\n",
            "조기 종료 카운터: 8 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.70it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 106.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [57] Train Loss : [0.01251] Val Loss : [0.01229] PSFA Score : [0.57155]\n",
            "조기 종료 카운터: 9 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.24it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 135.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [58] Train Loss : [0.01251] Val Loss : [0.01224] PSFA Score : [0.57028]\n",
            "조기 종료 카운터: 10 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.06it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [59] Train Loss : [0.01250] Val Loss : [0.01226] PSFA Score : [0.56481]\n",
            "조기 종료 카운터: 11 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.79it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 108.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [60] Train Loss : [0.01249] Val Loss : [0.01284] PSFA Score : [0.56030]\n",
            "조기 종료 카운터: 12 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.16it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 138.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [61] Train Loss : [0.01249] Val Loss : [0.01260] PSFA Score : [0.57327]\n",
            "조기 종료 카운터: 13 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.47it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [62] Train Loss : [0.01248] Val Loss : [0.01246] PSFA Score : [0.57284]\n",
            "조기 종료 카운터: 14 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.62it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 110.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [63] Train Loss : [0.01249] Val Loss : [0.01257] PSFA Score : [0.57433]\n",
            "조기 종료 카운터: 15 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.49it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 133.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [64] Train Loss : [0.01248] Val Loss : [0.01219] PSFA Score : [0.56737]\n",
            "조기 종료 카운터: 16 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.09it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 140.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [65] Train Loss : [0.01247] Val Loss : [0.01220] PSFA Score : [0.56870]\n",
            "조기 종료 카운터: 17 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.67it/s]\n",
            "100%|██████████| 678/678 [00:06<00:00, 105.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [66] Train Loss : [0.01247] Val Loss : [0.01321] PSFA Score : [0.57713]\n",
            "조기 종료 카운터: 18 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:38<00:00, 70.88it/s]\n",
            "100%|██████████| 678/678 [00:05<00:00, 123.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [67] Train Loss : [0.01247] Val Loss : [0.01235] PSFA Score : [0.57382]\n",
            "조기 종료 카운터: 19 / 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2709/2709 [00:39<00:00, 69.46it/s]\n",
            "100%|██████████| 678/678 [00:04<00:00, 138.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [68] Train Loss : [0.01247] Val Loss : [0.01231] PSFA Score : [0.56617]\n",
            "조기 종료 카운터: 20 / 20\n",
            "Early stopping\n",
            "Best saved validation loss: 0.012279\n",
            " Best PSFA Score : [0.57713]\n",
            " Best Loss Score : [0.01219]\n"
          ]
        }
      ],
      "source": [
        "# epoch 설정\n",
        "save_model_loss = None  # 저장할 모델의 손실값 초기화\n",
        "\n",
        "# Model과 Optimizer 설정\n",
        "model = BaseModel()  # 기본 모델 인스턴스 생성\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=CFG[\"LEARNING_RATE\"])  # AdamW 옵티마이저 사용\n",
        "\n",
        "# Early Stopping 객체 생성\n",
        "early_stopping = EarlyStopping(patience=20, verbose=True, path=path + 'Model_earlystop.pt')  # 일찍 멈추기 설정 (20회 동안 개선이 없으면 훈련 중단)\n",
        "\n",
        "# 훈련 함수 호출 및 최고 점수 반환 받기\n",
        "best_loss, best_psfa = train(model, optimizer, train_loader, val_loader, device)  # 훈련 실행하고 최고의 손실값과 PSFA 점수를 가져옴\n",
        "\n",
        "# 최고 점수 출력\n",
        "print(f' Best PSFA Score : [{best_psfa:.5f}]')  # 최고 PSFA 점수 출력\n",
        "print(f' Best Loss Score : [{best_loss:.5f}]')  # 최고 손실 점수 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cWWdKTeC_t7x",
      "metadata": {
        "id": "cWWdKTeC_t7x"
      },
      "source": [
        "### 추론 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QPnJiaXi_t75",
      "metadata": {
        "id": "QPnJiaXi_t75"
      },
      "source": [
        "#### Load Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "XlAL3MHjqxXc",
      "metadata": {
        "id": "XlAL3MHjqxXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da67a6a3-0b08-4914-fa32-0adb6b7234f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModel(\n",
              "  (lstm): LSTM(7, 256, batch_first=True)\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=128, out_features=21, bias=True)\n",
              "  )\n",
              "  (actv): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# 동일한 구조의 모델 객체를 생성\n",
        "infer_model = BaseModel()  # 추론(inference)을 위한 새 모델 객체 생성\n",
        "\n",
        "# 가장 좋은 RSFA 모델의 가중치를 불러오기 위한 경로 설정\n",
        "best_model_rsfa_path = path + 'Model(best)_Loss.pt'  # 가장 좋은 RSFA 모델의 저장 경로\n",
        "\n",
        "# 미리 훈련된 모델의 가중치 불러오기\n",
        "infer_model.load_state_dict(torch.load(best_model_rsfa_path))  # 훈련된 가중치를 새 모델에 로드\n",
        "\n",
        "# 모델을 디바이스(GPU 또는 CPU)로 이동\n",
        "infer_model.to(device)  # 모델을 실행 디바이스로 옮김 (GPU)\n",
        "\n",
        "# 모델을 추론 모드로 설정\n",
        "infer_model.eval()  # 모델을 평가 모드로 전환 (Dropout, BatchNorm 등을 고정)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HbIYz60K_t75",
      "metadata": {
        "id": "HbIYz60K_t75"
      },
      "source": [
        "#### Make Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "taIMOIKc_t75",
      "metadata": {
        "id": "taIMOIKc_t75"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(test_input, test_encoding, None, test_indices)\n",
        "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efWgB2tR_t75",
      "metadata": {
        "id": "efWgB2tR_t75"
      },
      "source": [
        "#### Inference & Inverse Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suUvqqqM_t75",
      "metadata": {
        "id": "suUvqqqM_t75"
      },
      "source": [
        "* Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "HmJr6YbD_t76",
      "metadata": {
        "id": "HmJr6YbD_t76"
      },
      "outputs": [],
      "source": [
        "# 모델을 사용하여 테스트 데이터에 대한 추론을 수행하는 함수 정의\n",
        "def inference(model, test_loader, device):\n",
        "    predictions = []  # 예측값을 저장할 리스트\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 테스트 로더에서 배치 단위로 데이터 가져오기\n",
        "        for data_input, data_encoding, _ in tqdm(test_loader):  # '_'는 사용하지 않는 index 값을 무시하기 위해 사용\n",
        "            data_input = data_input.to(device)\n",
        "            data_encoding = data_encoding.to(device)  # 데이터를 실행 디바이스로 옮김 (GPU)\n",
        "            output = model(data_input, data_encoding)  # 모델을 통해 예측 수행\n",
        "            output = output.cpu().numpy()  # 예측 결과를 Numpy 배열로 변환\n",
        "            predictions.extend(output)  # 예측 결과를 리스트에 추가\n",
        "\n",
        "    return np.array(predictions)  # Numpy 배열로 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3hM__S9L_t76",
      "metadata": {
        "id": "3hM__S9L_t76"
      },
      "source": [
        "* Run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "m7INzCM9_t76",
      "metadata": {
        "id": "m7INzCM9_t76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e0468d-5cef-43ea-ea78-8f06b866b0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 113/113 [00:00<00:00, 139.67it/s]\n"
          ]
        }
      ],
      "source": [
        "# 추론 함수 호출\n",
        "pred = inference(infer_model, test_loader, device)\n",
        "\n",
        "# 추론 결과에 역정규화 (inverse scaling) 적용\n",
        "for idx in range(len(pred)):\n",
        "    pred[idx, :] = pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
        "\n",
        "# 결과 후처리: 소수점 반올림 후 정수로 변환\n",
        "pred = np.round(pred, 0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wiNuJxVf_t76",
      "metadata": {
        "id": "wiNuJxVf_t76"
      },
      "source": [
        "#### Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "t8xo7YlR_t76",
      "metadata": {
        "id": "t8xo7YlR_t76"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('data/sample_submission.csv')\n",
        "submit.head()\n",
        "\n",
        "submit.iloc[:,1:] = pred\n",
        "submit.head()\n",
        "\n",
        "submit.to_csv('./LSTM_fc_best+.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaJ5YJV_y61G"
      },
      "id": "DaJ5YJV_y61G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bnnwMWUs_E0u",
        "x7WqL0H3p9R7",
        "czdojZpYrByf",
        "k7FZQ-IarBTM",
        "b1CuXdmerAev",
        "56af1fd9",
        "51c35f8e",
        "6005c3bb",
        "1a75e5df",
        "lSpUgfWw_Id2",
        "fV0lSgZz_GDm",
        "PpEpp578_NV2",
        "KnCZG3vOWj-2",
        "cWWdKTeC_t7x",
        "QPnJiaXi_t75",
        "HbIYz60K_t75",
        "efWgB2tR_t75",
        "wiNuJxVf_t76"
      ],
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}